{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bbd73608-9575-4dd5-8122-69fb4f7b6bd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tantri/Documents/My Research Material/UTSSpring23/AdvancedML/AT2/Retail_Analytics/American_Retail_Sales/notebooks\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3bf68037-09eb-4699-ad7b-ab7aa1146b94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the preprocessed data from a CSV file\n",
    "data = pd.read_csv('../data/processed/final_merged_events.csv', low_memory=False)\n",
    "\n",
    "# Pseudocode explanation:\n",
    "# 1. Import the required libraries (pandas and numpy).\n",
    "# 2. Load the preprocessed data from the 'final_merged_events.csv' file.\n",
    "# 3. The 'low_memory=False' parameter is used to read the CSV file efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af98cb0c-ba71-46a0-a2a6-8150e2e2eda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#not storing this in a data_cleaned df variable as the dataset is very large. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d2f9b4d-0516-4bd8-aedf-08d1c890d460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required library\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Pseudocode explanation:\n",
    "# 1. Import the Pipeline class from scikit-learn.\n",
    "# 2. A pipeline is a sequence of data processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9502daf1-9db8-474d-b2e3-e7531d6d5751",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List of store names\n",
    "store_names = ['CA_1', 'CA_2', 'CA_3', 'CA_4', 'TX_1', 'TX_2', 'TX_3', 'WI_1', 'WI_2', 'WI_3']\n",
    "\n",
    "# Define the groups\n",
    "group1 = ['CA_1', 'CA_2', 'CA_3', 'CA_4']\n",
    "group2 = ['TX_1', 'TX_2', 'TX_3']\n",
    "group3 = ['WI_1', 'WI_2', 'WI_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d45cc75b-a700-4976-a94e-2dbda4ada076",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032414 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 260\n",
      "[LightGBM] [Info] Number of data points in the train set: 12028182, number of used features: 2\n",
      "[LightGBM] [Info] Start training from score 3.410668\n",
      "TargetEncoder(cols=['store_id', 'item_id'])\n",
      "Test RMSE for the group1: 8.023665513740387\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026872 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 259\n",
      "[LightGBM] [Info] Number of data points in the train set: 9021136, number of used features: 2\n",
      "[LightGBM] [Info] Start training from score 2.929159\n",
      "TargetEncoder(cols=['store_id', 'item_id'])\n",
      "Test RMSE for the group2: 7.089358911886424\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027223 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 259\n",
      "[LightGBM] [Info] Number of data points in the train set: 9021136, number of used features: 2\n",
      "[LightGBM] [Info] Start training from score 2.629241\n",
      "TargetEncoder(cols=['store_id', 'item_id'])\n",
      "Test RMSE for the group3: 6.576111945901974\n",
      "Average RMSE across groups: 7.229712123842929\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "\n",
    "# Define a custom transformer for feature engineering\n",
    "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X['date'] = pd.to_datetime(X['date'])\n",
    "\n",
    "        # Extract date components and create new features\n",
    "        X['day_of_week'] = X['date'].dt.dayofweek\n",
    "        X['month'] = X['date'].dt.month\n",
    "        X['year'] = X['date'].dt.year\n",
    "\n",
    "        # Drop the original 'date' column if needed\n",
    "        # X = X.drop(columns=['date'])\n",
    "\n",
    "        return X\n",
    "\n",
    "# Define the LightGBM model\n",
    "model = lgb.LGBMRegressor()\n",
    "\n",
    "# Define the categorical and date feature groups\n",
    "date_features = ['day_of_week', 'month', 'year']\n",
    "categorical_features = ['store_id', 'item_id']\n",
    "\n",
    "# Define a pipeline for target encoding\n",
    "\n",
    "# Define the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('feature_engineer', FeatureEngineer(), []),\n",
    "        ('target_encoder', TargetEncoder(),categorical_features)  # Apply target encoding\n",
    "    ] \n",
    ")\n",
    "\n",
    "# Create the final pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),  # Preprocessing steps\n",
    "    ('model', model)  # LightGBM model\n",
    "])\n",
    "\n",
    "# Initialize lists to store RMSE for each group\n",
    "rmse_scores = []\n",
    "\n",
    "# Iterate over each group and filter the data accordingly\n",
    "for group_idx, group in enumerate([group1, group2, group3]):\n",
    "    # Filter the data for the current group\n",
    "    group_data = data[data['store_id'].isin(group)]\n",
    "\n",
    "    # Define the target variable\n",
    "    target = 'sales'\n",
    "\n",
    "    # Split the data into training, validation, and test sets\n",
    "    train_data, test_data = train_test_split(group_data, test_size=0.2, random_state=42)\n",
    "    train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Extract features and target variables for training, validation, and test sets\n",
    "    X_train, y_train = train_data.drop(columns=[target]), train_data[target]\n",
    "    X_val, y_val = val_data.drop(columns=[target]), val_data[target]\n",
    "    X_test, y_test = test_data.drop(columns=[target]), test_data[target]\n",
    "\n",
    "    # Fit the pipeline to the training data\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    test_predictions = pipeline.predict(X_test)\n",
    "    encoder = pipeline.named_steps['preprocessor'].named_transformers_['target_encoder']\n",
    "    #encoder = pipeline.named_steps['target_encoder']\n",
    "\n",
    "    print (encoder)\n",
    "    model_and_encoders = {\n",
    "        'model': pipeline,\n",
    "        'encoders': encoder\n",
    "    }\n",
    "\n",
    "    models_dir = \"../models/Predictive\"  # Update the directory name\n",
    "    os.makedirs(models_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "    # Define the model file path for saving\n",
    "    model_file_path = os.path.join(models_dir, f'model_group_{group_idx + 1}.joblib')\n",
    "\n",
    "    # Save the model to the specified file path using joblib\n",
    "    joblib.dump(model_and_encoders, model_file_path)\n",
    "\n",
    "\n",
    "    # Calculate RMSE on the test set\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, test_predictions))\n",
    "    print(f'Test RMSE for the group{group_idx + 1}: {rmse}')\n",
    "\n",
    "    # Append the RMSE score to the list\n",
    "    rmse_scores.append(rmse)\n",
    "\n",
    "# Calculate the average RMSE across groups\n",
    "average_rmse = np.mean(rmse_scores)\n",
    "print(f'Average RMSE across groups: {average_rmse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "12b94503-9088-467b-aef3-d922ec5a15c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "test_data = pd.read_csv('../data/processed/test_final_merged.csv', low_memory=False)\n",
    "\n",
    "test_data['date'] = pd.to_datetime(test_data['date'])\n",
    "\n",
    "# Now you can use .dt accessor to extract date components\n",
    "test_data['day_of_week'] = test_data['date'].dt.dayofweek\n",
    "test_data['month'] = test_data['date'].dt.month\n",
    "test_data['year'] = test_data['date'].dt.year\n",
    "test_data_2016 = test_data[test_data['year'] == 2016]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7b378bcf-ad2a-4a21-b510-f618122ebe45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'store_id': TargetEncoder(cols=['store_id']), 'item_id': TargetEncoder(cols=['item_id'])}\n",
      "LightGBM RMSE for 2016-01-08 00:00:00 in CA_2: 6.325721139266072\n",
      "{'store_id': TargetEncoder(cols=['store_id']), 'item_id': TargetEncoder(cols=['item_id'])}\n",
      "LightGBM RMSE for 2016-05-19 00:00:00 in WI_3: 7.385783287405409\n",
      "{'store_id': TargetEncoder(cols=['store_id']), 'item_id': TargetEncoder(cols=['item_id'])}\n",
      "LightGBM RMSE for 2016-03-12 00:00:00 in CA_2: 1.057343191444116\n",
      "{'store_id': TargetEncoder(cols=['store_id']), 'item_id': TargetEncoder(cols=['item_id'])}\n",
      "LightGBM RMSE for 2016-01-22 00:00:00 in TX_2: 2.666413111623605\n",
      "{'store_id': TargetEncoder(cols=['store_id']), 'item_id': TargetEncoder(cols=['item_id'])}\n",
      "LightGBM RMSE for 2016-04-21 00:00:00 in WI_3: 1.9363434518271796\n",
      "{'store_id': TargetEncoder(cols=['store_id']), 'item_id': TargetEncoder(cols=['item_id'])}\n",
      "LightGBM RMSE for 2016-03-30 00:00:00 in TX_3: 17.133841770993694\n",
      "{'store_id': TargetEncoder(cols=['store_id']), 'item_id': TargetEncoder(cols=['item_id'])}\n",
      "LightGBM RMSE for 2016-01-07 00:00:00 in WI_1: 0.6055259332508349\n",
      "{'store_id': TargetEncoder(cols=['store_id']), 'item_id': TargetEncoder(cols=['item_id'])}\n",
      "LightGBM RMSE for 2016-02-25 00:00:00 in CA_3: 2.557523666094589\n",
      "{'store_id': TargetEncoder(cols=['store_id']), 'item_id': TargetEncoder(cols=['item_id'])}\n",
      "LightGBM RMSE for 2016-03-15 00:00:00 in TX_3: 1.3940255086595412\n",
      "{'store_id': TargetEncoder(cols=['store_id']), 'item_id': TargetEncoder(cols=['item_id'])}\n",
      "LightGBM RMSE for 2016-01-13 00:00:00 in CA_2: 1.5576358061426416\n",
      "{'store_id': TargetEncoder(cols=['store_id']), 'item_id': TargetEncoder(cols=['item_id'])}\n",
      "LightGBM RMSE for 2016-03-30 00:00:00 in TX_3: 0.48917607508020966\n",
      "{'store_id': TargetEncoder(cols=['store_id']), 'item_id': TargetEncoder(cols=['item_id'])}\n",
      "LightGBM RMSE for 2016-04-29 00:00:00 in TX_3: 7.124411997015962\n",
      "{'store_id': TargetEncoder(cols=['store_id']), 'item_id': TargetEncoder(cols=['item_id'])}\n",
      "LightGBM RMSE for 2016-03-15 00:00:00 in CA_4: 1.3623808518457319\n",
      "{'store_id': TargetEncoder(cols=['store_id']), 'item_id': TargetEncoder(cols=['item_id'])}\n",
      "LightGBM RMSE for 2016-02-01 00:00:00 in TX_3: 1.4599321540509398\n",
      "{'store_id': TargetEncoder(cols=['store_id']), 'item_id': TargetEncoder(cols=['item_id'])}\n",
      "LightGBM RMSE for 2016-02-10 00:00:00 in CA_2: 0.2866309093034487\n",
      "{'store_id': TargetEncoder(cols=['store_id']), 'item_id': TargetEncoder(cols=['item_id'])}\n",
      "LightGBM RMSE for 2016-04-11 00:00:00 in WI_2: 2.4563853753462563\n",
      "{'store_id': TargetEncoder(cols=['store_id']), 'item_id': TargetEncoder(cols=['item_id'])}\n",
      "LightGBM RMSE for 2016-05-12 00:00:00 in CA_3: 4.188609014323193\n",
      "{'store_id': TargetEncoder(cols=['store_id']), 'item_id': TargetEncoder(cols=['item_id'])}\n",
      "LightGBM RMSE for 2016-04-28 00:00:00 in WI_1: 17.846045586554016\n",
      "{'store_id': TargetEncoder(cols=['store_id']), 'item_id': TargetEncoder(cols=['item_id'])}\n",
      "LightGBM RMSE for 2016-03-11 00:00:00 in CA_1: 0.7346916116310873\n",
      "{'store_id': TargetEncoder(cols=['store_id']), 'item_id': TargetEncoder(cols=['item_id'])}\n",
      "LightGBM RMSE for 2016-01-29 00:00:00 in TX_3: 1.611709374169675\n",
      "{'store_id': TargetEncoder(cols=['store_id']), 'item_id': TargetEncoder(cols=['item_id'])}\n",
      "LightGBM RMSE for 2016-03-17 00:00:00 in WI_3: 0.47751447576460904\n",
      "{'store_id': TargetEncoder(cols=['store_id']), 'item_id': TargetEncoder(cols=['item_id'])}\n",
      "LightGBM RMSE for 2016-01-08 00:00:00 in TX_3: 5.761105761455823\n",
      "{'store_id': TargetEncoder(cols=['store_id']), 'item_id': TargetEncoder(cols=['item_id'])}\n",
      "LightGBM RMSE for 2016-04-25 00:00:00 in TX_3: 3.0124180464465993\n",
      "{'store_id': TargetEncoder(cols=['store_id']), 'item_id': TargetEncoder(cols=['item_id'])}\n",
      "LightGBM RMSE for 2016-01-03 00:00:00 in WI_3: 0.14275922984742262\n",
      "{'store_id': TargetEncoder(cols=['store_id']), 'item_id': TargetEncoder(cols=['item_id'])}\n",
      "LightGBM RMSE for 2016-01-09 00:00:00 in TX_3: 8.493545108247915\n",
      "{'store_id': TargetEncoder(cols=['store_id']), 'item_id': TargetEncoder(cols=['item_id'])}\n",
      "LightGBM RMSE for 2016-02-15 00:00:00 in WI_1: 8.150265231937594\n",
      "{'store_id': TargetEncoder(cols=['store_id']), 'item_id': TargetEncoder(cols=['item_id'])}\n",
      "LightGBM RMSE for 2016-01-16 00:00:00 in WI_2: 1.6402611565808285\n",
      "{'store_id': TargetEncoder(cols=['store_id']), 'item_id': TargetEncoder(cols=['item_id'])}\n",
      "LightGBM RMSE for 2016-03-01 00:00:00 in WI_2: 3.3570270038552463\n",
      "{'store_id': TargetEncoder(cols=['store_id']), 'item_id': TargetEncoder(cols=['item_id'])}\n",
      "LightGBM RMSE for 2016-01-13 00:00:00 in CA_4: 4.884243829773837\n",
      "{'store_id': TargetEncoder(cols=['store_id']), 'item_id': TargetEncoder(cols=['item_id'])}\n",
      "LightGBM RMSE for 2016-05-20 00:00:00 in CA_2: 1.0954545812470409\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define the store names and groups\n",
    "store_names = ['CA_1', 'CA_2', 'CA_3', 'CA_4', 'TX_1', 'TX_2', 'TX_3', 'WI_1', 'WI_2', 'WI_3']\n",
    "group1 = ['CA_1', 'CA_2', 'CA_3', 'CA_4']\n",
    "group2 = ['TX_1', 'TX_2', 'TX_3']\n",
    "group3 = ['WI_1', 'WI_2', 'WI_3']\n",
    "\n",
    "# Filter the data for the year 2016\n",
    "test_data_2016 = test_data[test_data['year'] == 2016]\n",
    "\n",
    "# Define the number of random days to select\n",
    "num_random_days = 30\n",
    "\n",
    "# Get a random sample of day indices from the 2016 data\n",
    "random_day_indices = random.sample(test_data_2016.index.tolist(), num_random_days)\n",
    "\n",
    "# Iterate through each randomly selected day\n",
    "for index in random_day_indices:\n",
    "    row = test_data_2016.loc[index]\n",
    "    \n",
    "    # Determine the model_group based on the store_id\n",
    "    store_id = row['store_id']\n",
    "    if store_id in group1:\n",
    "        model_group = 1\n",
    "    elif store_id in group2:\n",
    "        model_group = 2\n",
    "    elif store_id in group3:\n",
    "        model_group = 3\n",
    "    else:\n",
    "        # Handle the case when the store_id doesn't match any group\n",
    "        model_group = None\n",
    "\n",
    "    if model_group is not None:\n",
    "        # Load the trained model for the determined model_group\n",
    "        model_file_path = f\"../models/LightGBM/model_group_{model_group}.joblib\"\n",
    "        loaded_model = joblib.load(model_file_path)\n",
    "        print(loaded_model['encoders'])\n",
    "        # Prepare the input data for batch prediction\n",
    "        input_data = pd.DataFrame({\n",
    "            'day_of_week': [row['day_of_week']],\n",
    "            'month': [row['month']],\n",
    "            'year': [row['year']],\n",
    "            'store_id': [row['store_id']],\n",
    "            'item_id': [row['item_id']]\n",
    "        })\n",
    "\n",
    "        # Perform target encoding on categorical features using the loaded encoders\n",
    "        for feature, encoder in loaded_model['encoders'].items():\n",
    "            input_data[feature] = encoder.transform(input_data[feature])\n",
    "\n",
    "        # Make batch predictions using the loaded model\n",
    "        predicted_sales_batch = loaded_model['model'].predict(input_data)\n",
    "\n",
    "        # Calculate the squared difference for the batch\n",
    "        squared_diff_batch = (row['sales'] - predicted_sales_batch[0]) ** 2\n",
    "\n",
    "        # Calculate the RMSE for the selected random day\n",
    "        rmse_batch = np.sqrt(squared_diff_batch)\n",
    "\n",
    "        # Print the RMSE for the selected random day\n",
    "        print(f\"LightGBM RMSE for {row['date']} in {row['store_id']}: {rmse_batch}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7168398d-a713-4484-8f6e-94e846ac9d75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
